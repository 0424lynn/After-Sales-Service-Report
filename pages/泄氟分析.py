# ==== æœ¬é¡µéšè—ä¾§æ ï¼ˆæ›´å¼ºå…¼å®¹ç‰ˆï¼‰+ æ”¾å¤§å­—ä½“ ==== 
import streamlit as st
import streamlit.components.v1 as components

# åªä¿ç•™ä¸€æ¬¡ set_page_configï¼Œæ”¾æœ€å‰ï¼ˆè‹¥ä¸Šå±‚å·²è®¾ç½®åˆ™å¿½ç•¥ï¼‰
try:
    st.set_page_config(page_title="æ³„æ¼åˆ†æï¼ˆ1.1.3.1ï¼‰", layout="wide", initial_sidebar_state="collapsed")
except Exception:
    pass

# â€”â€” CSS & JSï¼ˆéšè—ä¾§æ ï¼‰
st.markdown("""
<style>
aside[aria-label="sidebar"],aside[aria-label="Sidebar"],aside[class*="sidebar"],
[data-testid="stSidebar"],[data-testid^="stSidebar"],[data-testid*="Sidebar"],
[data-testid="stSidebarNav"],[data-testid="stSidebarContent"],[data-testid="stSidebarCollapsedControl"],
[data-testid*="Collapse"],[data-testid*="collapsed"],nav[aria-label="Sidebar navigation"]{
  display:none!important;width:0!important;min-width:0!important;max-width:0!important;
  visibility:hidden!important;pointer-events:none!important;opacity:0!important;
}
div.block-container{padding-left:1rem!important;padding-right:1rem!important;margin-left:0!important;}
section.main,div.main,[data-testid="stAppViewContainer"]{margin-left:0!important;}
</style>
""", unsafe_allow_html=True)

components.html("""
<script>
(function(){
  function nuke(){
    const sels=[
      'aside[aria-label="sidebar"]','aside[aria-label="Sidebar"]','aside[class*="sidebar"]',
      '[data-testid="stSidebar"]','[data-testid^="stSidebar"]','[data-testid*="Sidebar"]',
      '[data-testid="stSidebarNav"]','[data-testid="stSidebarContent"]','[data-testid="stSidebarCollapsedControl"]',
      '[data-testid*="Collapse"]','[data-testid*="collapsed"]','nav[aria-label="Sidebar navigation"]'
    ];
    for(const s of sels){
      document.querySelectorAll(s).forEach(el=>{
        el.style.display='none';el.style.width='0';el.style.minWidth='0';el.style.maxWidth='0';
        el.style.visibility='hidden';el.style.pointerEvents='none';el.style.opacity='0';
      });
    }
    const bc=document.querySelector('div.block-container');
    if(bc){bc.style.paddingLeft='1rem';bc.style.paddingRight='1rem';}
  }
  nuke();
  new MutationObserver(nuke).observe(document.body,{subtree:true,childList:true,attributes:true});
  setInterval(nuke,500);
})();
</script>
""", height=0)

# ================= åŸºç¡€ä¾èµ– =================
import pandas as pd
import numpy as np
import re, json
from pathlib import Path
from urllib.parse import unquote

# ================= é¡µé¢æŠ¬å¤´ =================
try:
    st.set_page_config(page_title="æ³„æ¼é…ä»¶åˆ†æï¼ˆ1.1.3.1ï¼‰", layout="wide", initial_sidebar_state="expanded")
except Exception:
    pass
st.title("æ³„æ¼é…ä»¶åˆ†æ 1.1.3.1")

# ================= æŒä¹…åŒ–è·¯å¾„ =================
STORE_PARQUET     = Path("data_store.parquet")
STORE_CSV         = Path("data_store.csv")
MODEL_FLEET_JSON  = Path("model_fleet_counts.json")   # å‹å·ç³»åˆ—åœ¨ä¿é‡ï¼ˆå¯é€‰ï¼‰
SPEC_FLEET_JSON   = Path("spec_fleet_counts.json")    # âœ… å…·ä½“å‹å·åœ¨ä¿é‡ï¼ˆé€šç”¨ï¼‰
RAW_EXCEL_STORE   = Path("raw_excel_store.parquet")   # â€œå†°ç®±æ•°æ®â€é¡µçš„åŸå§‹ Excel åº“

def _load_raw_excel_store() -> pd.DataFrame:
    if RAW_EXCEL_STORE.exists():
        try:
            return pd.read_parquet(RAW_EXCEL_STORE)
        except Exception:
            try:
                return pd.read_csv(RAW_EXCEL_STORE.with_suffix(".csv"), dtype=str)
            except Exception:
                pass
    return pd.DataFrame()

# ============== è§£æå·¥å…· ==============
PROD_DATE_CANDS = ["ç”Ÿäº§æ—¥æœŸ","ç”Ÿäº§æ—¥","productiondate","proddate","åˆ¶é€ æ—¥æœŸ","å‡ºå‚æ—¥æœŸ"]

# === æ–°å¢ï¼šç»Ÿä¸€æ ¼å¼åŒ–å·¥å…· ===
def normalize_prod_date_series_to_text(raw: pd.Series) -> pd.Series:
    s = raw.astype(str).str.strip()
    out = pd.Series("", index=s.index, dtype="object")

    m8 = s.str.fullmatch(r"\d{8}", na=False)
    if m8.any():
        dt8 = pd.to_datetime(s[m8], format="%Y%m%d", errors="coerce")
        out.loc[m8] = dt8.dt.strftime("%Y-%m-%d")

    m6 = s.str.fullmatch(r"\d{6}", na=False)
    if m6.any():
        s6 = s[m6]
        is_yyyymm = s6.str.match(r"(19|20)\d{2}(0[1-9]|1[0-2])$")
        idx_y4 = s6.index[is_yyyymm.fillna(False)]
        if len(idx_y4) > 0:
            dt_a = pd.to_datetime(s6.loc[idx_y4] + "01", format="%Y%m%d", errors="coerce")
            out.loc[idx_y4] = dt_a.dt.strftime("%Y-%m-%d")
        idx_y2 = s6.index[~is_yyyymm.fillna(False)]
        if len(idx_y2) > 0:
            y = "20" + s6.loc[idx_y2].str.slice(0, 2)
            m = s6.loc[idx_y2].str.slice(2, 4)
            d = s6.loc[idx_y2].str.slice(4, 6)
            dt_b = pd.to_datetime(y + m + d, format="%Y%m%d", errors="coerce")
            out.loc[idx_y2] = dt_b.dt.strftime("%Y-%m-%d")

    rest = (out == "")
    if rest.any():
        dt = pd.to_datetime(s[rest], errors="coerce", infer_datetime_format=True)
        out.loc[rest] = dt.dt.strftime("%Y-%m-%d")

    failed = out.isna() | (out == "NaT") | (out == "")
    if failed.any():
        out.loc[failed] = s.loc[failed]
    return out

def _parse_prod_year_series(s: pd.Series) -> pd.Series:
    s = s.astype(str).str.strip()
    y = pd.Series(np.nan, index=s.index, dtype="float")

    m8 = s.str.fullmatch(r"\d{8}", na=False)
    if m8.any():
        y.loc[m8] = s.loc[m8].str.slice(0, 4).astype(int)

    m6 = s.str.fullmatch(r"\d{6}", na=False)
    if m6.any():
        s6 = s.loc[m6]
        is_yyyymm = s6.str.match(r"(19|20)\d{2}(0[1-9]|1[0-2])$")
        if is_yyyymm.any():
            y.loc[s6.index[is_yyyymm]] = s6[is_yyyymm].str.slice(0, 4).astype(int)
        idx_y2 = s6.index[~is_yyyymm.fillna(False)]
        if len(idx_y2) > 0:
            y.loc[idx_y2] = 2000 + s6.loc[idx_y2].str.slice(0, 2).astype(int)

    rest_idx = y.index[y.isna()]
    if len(rest_idx) > 0:
        dt2 = pd.to_datetime(s.loc[rest_idx], errors="coerce", infer_datetime_format=True)
        ok_idx = dt2.index[dt2.notna()]
        if len(ok_idx) > 0:
            y.loc[ok_idx] = dt2.loc[ok_idx].dt.year.astype(int)
    return y

def _parse_prod_ym_series(s: pd.Series):
    s = s.astype(str).str.strip()
    year = pd.Series(np.nan, index=s.index, dtype="float")
    month = pd.Series(np.nan, index=s.index, dtype="float")

    m8 = s.str.fullmatch(r"\d{8}", na=False)
    if m8.any():
        year.loc[m8]  = s.loc[m8].str.slice(0, 4).astype(int)
        month.loc[m8] = s.loc[m8].str.slice(4, 6).astype(int)

    m6 = s.str.fullmatch(r"\d{6}", na=False)
    if m6.any():
        s6 = s.loc[m6]
        is_yyyymm = s6.str.match(r"(19|20)\d{2}(0[1-9]|1[0-2])$")
        if is_yyyymm.any():
            year.loc[s6.index[is_yyyymm]]  = s6[is_yyyymm].str.slice(0, 4).astype(int)
            month.loc[s6.index[is_yyyymm]] = s6[is_yyyymm].str.slice(4, 6).astype(int)
        idx_y2 = s6.index[~is_yyyymm.fillna(False)]
        if len(idx_y2) > 0:
            year.loc[idx_y2]  = 2000 + s6.loc[idx_y2].str.slice(0, 2).astype(int)
            month.loc[idx_y2] = s6.loc[idx_y2].str.slice(2, 4).astype(int)
    need_idx = year.index[year.isna() | month.isna()]
    if len(need_idx) > 0:
        dt2 = pd.to_datetime(s.loc[need_idx], errors="coerce", infer_datetime_format=True)
        ok_idx = dt2.index[dt2.notna()]
        if len(ok_idx) > 0:
            year.loc[ok_idx]  = dt2.loc[ok_idx].dt.year.astype(int)
            month.loc[ok_idx] = dt2.loc[ok_idx].dt.month.astype(int)
    return year, month

def parse_any_prod_date_series(raw: pd.Series) -> pd.Series:
    s = raw.astype(str).str.strip()
    out = pd.Series(pd.NaT, index=s.index, dtype="datetime64[ns]")

    m8 = s.str.fullmatch(r"\d{8}", na=False)
    if m8.any():
        out.loc[m8] = pd.to_datetime(s[m8], format="%Y%m%d", errors="coerce")

    m6 = s.str.fullmatch(r"\d{6}", na=False)
    if m6.any():
        s6 = s[m6]
        is_yyyymm = s6.str.slice(0, 4).astype(int) >= 1900
        idx_a = s6.index[is_yyyymm]
        if len(idx_a) > 0:
            out.loc[idx_a] = pd.to_datetime(s6.loc[idx_a] + "01", format="%Y%m%d", errors="coerce")
        idx_b = s6.index[~is_yyyymm]
        if len(idx_b) > 0:
            y = "20" + s6.loc[idx_b].str.slice(0, 2)
            m = s6.loc[idx_b].str.slice(2, 4)
            d = s6.loc[idx_b].str.slice(4, 6)
            out.loc[idx_b] = pd.to_datetime(y + m + d, format="%Y%m%d", errors="coerce")

    rest = out.isna()
    if rest.any():
        out.loc[rest] = pd.to_datetime(s[rest], errors="coerce", infer_datetime_format=True)
    return out

# === ä¿ç•™ï¼šå…·ä½“å‹å·åœ¨ä¿é‡æ˜ å°„ ===
def normalize_model_key(s: str) -> str:
    s = "" if s is None else str(s)
    s = s.replace("\u3000"," ")
    s = re.sub(r"\s+"," ",s).strip()
    return s.upper()

def _load_spec_fleet_any():
    if not SPEC_FLEET_JSON.exists(): return [],{}
    try:
        data = json.loads(SPEC_FLEET_JSON.read_text(encoding="utf-8"))
    except Exception:
        return [],{}
    if isinstance(data, dict) and "triples" in data:
        triples = data["triples"]
    elif isinstance(data, dict):
        triples = [{"a":k,"b":"", "fleet":v} for k,v in data.items()]
    elif isinstance(data, list):
        triples = data
    else:
        triples = []
    mp={}
    for r in triples:
        a = normalize_model_key(r.get("a",""))
        b = normalize_model_key(r.get("b",""))
        f = int(pd.to_numeric(r.get("fleet",0), errors="coerce") or 0)
        if a: mp[a]=f
        if b: mp[b]=f
    return triples, mp

if "spec_fleet_triples" not in st.session_state or "spec_fleet_map" not in st.session_state:
    triples, mp = _load_spec_fleet_any()
    st.session_state.spec_fleet_triples = triples
    st.session_state.spec_fleet_map = mp

def lookup_spec_fleet(model_name: str) -> int:
    return int(st.session_state.spec_fleet_map.get(normalize_model_key(model_name), 0))

# ================= å·¥å…·å‡½æ•°ï¼ˆæ¸…æ´—/æ ‡å‡†åŒ–ï¼‰ =================
def _normalize_df_date_column_inplace(df: pd.DataFrame, src_col: str, out_col: str = None):
    if df is None or df.empty or (src_col not in df.columns):
        return
    out_col = out_col or src_col
    df[out_col] = normalize_prod_date_series_to_text(df[src_col])

def ensure_cols(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=[
            "Date","Category1","Category2","TAG1","TAG2","TAG3","TAG4",
            "CostUSD","PaidQty","IsWarranty","Completed","Year","Month","Quarter","_SrcYM"
        ])

    mapping={}
    prod_src_col=None

    for c in df.columns:
        lc=str(c).strip().lower()
        if (lc in PROD_DATE_CANDS) and (prod_src_col is None):
            prod_src_col=c
        if lc in ["æ—¥æœŸ","date","created_at","å‘ç”Ÿæ—¥æœŸ"]:
            mapping[c]="Date"
        elif lc in ["åˆ†ç±»1","category1","å¤§ç±»"]:
            mapping[c]="Category1"
        elif lc in ["åˆ†ç±»2","category2","å°ç±»"]:
            mapping[c]="Category2"
        elif lc in ["tag1","tag 1","tag-1"]:
            mapping[c]="TAG1"
        elif lc in ["tag2","tag 2","tag-2"]:
            mapping[c]="TAG2"
        elif lc in ["tag3","tag 3","tag-3"]:
            mapping[c]="TAG3"
        elif lc in ["tag4","tag 4","tag-4"]:
            mapping[c]="TAG4"
        elif lc in ["cost","costusd","amount","è´¹ç”¨","é‡‘é¢","fee","cost(usd)","cost usd"]:
            mapping[c]="CostUSD"
        elif lc in ["paidqty","paid_qty","å·²ä»˜æ¬¾æ•°é‡"]:
            mapping[c]="PaidQty"
        elif lc in ["iswarranty","warranty","æ˜¯å¦æ‰¿ä¿"]:
            mapping[c]="IsWarranty"
        elif lc in ["completed","æ˜¯å¦å®Œæˆ","ç»´ä¿®çŠ¶æ€","çŠ¶æ€"]:
            mapping[c]="Completed"

    if mapping:
        df=df.rename(columns=mapping)

    if prod_src_col:
        df["åŸå§‹ç”Ÿäº§æ—¥æœŸ"] = df[prod_src_col]
        df["_NormProdDate"] = normalize_prod_date_series_to_text(df["åŸå§‹ç”Ÿäº§æ—¥æœŸ"]) 

    for col,default in [
        ("Category1",""),("Category2",""),("TAG1",""),("TAG2",""),("TAG3",""),("TAG4",""),
        ("CostUSD",0.0),("PaidQty",0),("IsWarranty",False),("Completed",False)
    ]:
        if col not in df.columns:
            df[col]=default

    df["CostUSD"]=pd.to_numeric(df["CostUSD"], errors="coerce").fillna(0.0)
    df["PaidQty"]=pd.to_numeric(df["PaidQty"], errors="coerce").fillna(0).astype(int)

    if prod_src_col:
        if "Year" not in df.columns or df["Year"].isna().all():
            yy,_ = _parse_prod_ym_series(df["_NormProdDate"])
            df["Year"]=yy
        if "Month" not in df.columns or df["Month"].isna().all():
            _,mm = _parse_prod_ym_series(df["_NormProdDate"])
            df["Month"]=mm

    if "Year" in df.columns:  df["Year"]=pd.to_numeric(df["Year"], errors="coerce")
    if "Month" in df.columns: df["Month"]=pd.to_numeric(df["Month"], errors="coerce")

    return df

def _clean_text(s: str) -> str:
    s = ("" if pd.isna(s) else str(s)).lower().replace("\u3000"," ")
    s = re.sub(r"[\s\-_\/]+"," ", s)
    return s.strip()

def get_param(name: str, default=None, cast=int):
    try:
        v = st.query_params.get(name, default)
    except Exception:
        qp = st.experimental_get_query_params()
        v = (qp.get(name,[default]) or [default])[0]
    if v is None: return default
    if cast is None: return v
    try: return cast(v)
    except Exception: return default

def get_param_str(name: str, default=None):
    try:
        v = st.query_params.get(name, default)
    except Exception:
        qp = st.experimental_get_query_params()
        v = (qp.get(name,[default]) or [default])[0]
    if isinstance(v,str): return unquote(v)
    return default

def load_store_df_from_disk()->pd.DataFrame:
    if STORE_PARQUET.exists():
        try: return pd.read_parquet(STORE_PARQUET)
        except Exception: pass
    if STORE_CSV.exists():
        try: return pd.read_csv(STORE_CSV)
        except Exception: pass
    return pd.DataFrame()

def get_store_df()->pd.DataFrame:
    if "store_df" in st.session_state and isinstance(st.session_state["store_df"], pd.DataFrame):
        df = st.session_state["store_df"].copy()
    else:
        df = load_store_df_from_disk()
    df = ensure_cols(df)
    if "åŸå§‹ç”Ÿäº§æ—¥æœŸ" in df.columns:
        df["åŸå§‹ç”Ÿäº§æ—¥æœŸ"] = normalize_prod_date_series_to_text(df["åŸå§‹ç”Ÿäº§æ—¥æœŸ"])
        df["_NormProdDate"] = df["åŸå§‹ç”Ÿäº§æ—¥æœŸ"]
    return df

def money(v)->str:
    try: return f"${float(v):,.0f}"
    except Exception: return "$0"

# ================= å‚æ•°ä¸æ•°æ®è¯»å– =================
code  = get_param_str("code", default="1.1.3.1")
year  = get_param("year", default=None, cast=int)
month = get_param("month", default=None, cast=int)
months_param = get_param_str("months", default=None)
q_param      = get_param_str("q", default=None)

df_all = get_store_df()
if df_all.empty:
    st.warning("æœªè·å–åˆ°ä»»ä½•æ•°æ®ï¼šè¯·å…ˆåœ¨â€œå†°ç®±æ•°æ®â€é¡µé¢ä¸Šä¼ æ•°æ®ï¼›æˆ–ç¡®è®¤ data_store.parquet / data_store.csv æ˜¯å¦å­˜åœ¨ã€‚")
    st.stop()

yy = pd.to_numeric(df_all.get("Year"), errors="coerce")
mm = pd.to_numeric(df_all.get("Month"), errors="coerce")

if year is None:
    if yy.notna().any():
        year = int(yy.dropna().max())
    else:
        year = 2025

sel_months = None
if months_param:
    try:
        sel_months = [int(x) for x in re.split(r"[,\s]+", months_param) if x]
    except Exception:
        sel_months = None
if (sel_months is None) and (month is not None):
    sel_months = [int(month)]
if (sel_months is None) and q_param:
    _q_map = {"Q1":[1,2,3], "Q2":[4,5,6], "Q3":[7,8,9], "Q4":[10,11,12]}
    sel_months = _q_map.get(str(q_param).upper(), None)
if sel_months is None:
    mm_this_year = pd.to_numeric(df_all.loc[yy == year, "Month"], errors="coerce")
    if mm_this_year.notna().any():
        sel_months = [int(mm_this_year.dropna().max())]
    else:
        sel_months = [1]

month_for_fleet = int(sorted(sel_months)[-1])

mask_year   = (yy == int(year))
mask_months = mm.isin([int(m) for m in sel_months])
scope_df    = df_all[mask_year & mask_months].copy()

_set = set(sel_months)
if _set == {1,2,3}:
    scope_text = f"{year} Q1"
elif _set == {4,5,6}:
    scope_text = f"{year} Q2"
elif _set == {7,8,9}:
    scope_text = f"{year} Q3"
elif _set == {10,11,12}:
    scope_text = f"{year} Q4"
elif len(sel_months) == 1:
    scope_text = f"{year}-{int(sel_months[0]):02d}"
else:
    scope_text = f"{year} æœˆä»½ï¼š{','.join(str(m) for m in sel_months)}"

if "_NormProdDate" not in scope_df.columns:
    cand = next((c for c in [
        "åŸå§‹ç”Ÿäº§æ—¥æœŸ","ç”Ÿäº§æ—¥æœŸ","ç”Ÿäº§æ—¥","productiondate","proddate","åˆ¶é€ æ—¥æœŸ","å‡ºå‚æ—¥æœŸ","Date"
    ] if c in scope_df.columns), None)
    if cand:
        scope_df["_NormProdDate"] = normalize_prod_date_series_to_text(scope_df[cand])

if "_NormProdDate" in scope_df.columns:
    py = _parse_prod_year_series(scope_df["_NormProdDate"])
    if py.notna().any():
        scope_df = scope_df.loc[py >= 2024].copy()
        st.caption("ï¼ˆç»Ÿè®¡ï¼šç”Ÿäº§å¹´ â‰¥ 2024ï¼‰")
    else:
        st.warning("æœªèƒ½ä»ã€åŸå§‹ç”Ÿäº§æ—¥æœŸã€‘è§£æå‡ºå¹´ä»½ï¼ˆå¯èƒ½æ—¥æœŸæ ¼å¼å¼‚å¸¸æˆ–åˆ—ä¸ºç©ºï¼‰ï¼Œæš‚ä¸è¿›è¡Œ â‰¥2024 è¿‡æ»¤ã€‚")
else:
    st.warning("æœªæ‰¾åˆ°ã€åŸå§‹ç”Ÿäº§æ—¥æœŸ/ç”Ÿäº§æ—¥æœŸ/Dateã€‘ç­‰å¯ç”¨äºè§£æçš„æ—¥æœŸåˆ—ï¼Œæš‚ä¸è¿›è¡Œ â‰¥2024 è¿‡æ»¤ã€‚")

if scope_df.empty:
    st.info(f"{scope_text} æ²¡æœ‰å¯ç»Ÿè®¡æ•°æ®ã€‚")
    st.stop()

# === åªåœ¨â€œç¼ºé™·ç åˆ—â€é‡ŒåŒ¹é… ===
def _detect_code_col(df: pd.DataFrame, code_root: str) -> str | None:
    if df is None or df.empty: return None
    pat = re.compile(rf'(?<!\d){re.escape(code_root)}(?:\.\d+)?(?!\d)', re.I)
    best_col, best_hit, best_ratio = None, 0, 0.0
    for c in df.columns:
        try: s = df[c].astype(str)
        except Exception: continue
        nn = (s != "").sum()
        if nn == 0: continue
        hit = s.str.contains(pat, na=False).sum()
        if hit == 0: continue
        ratio = hit / float(nn)
        if (hit > best_hit) or (hit == best_hit and ratio > best_ratio):
            best_col, best_hit, best_ratio = c, hit, ratio
    if best_col is None:
        candidates = ["ç¼ºé™·ç ", "ç¼ºé™·ç¼–ç ", "DefectCode", "FaultCode", "IssueCode", "é—®é¢˜ä»£ç ", "é—®é¢˜ç¼–ç "]
        for c in candidates:
            if c in df.columns:
                return c
    return best_col

code_root = (code or "1.1.3.1").strip()
code_col  = _detect_code_col(scope_df, code_root)
if code_col is None:
    st.warning("æœªæ‰¾åˆ°æ˜æ˜¾çš„â€œç¼ºé™·ç â€åˆ—ã€‚è¯·æ£€æŸ¥åŸå§‹æ•°æ®ä¸­æ˜¯å¦æœ‰æ˜ç¡®çš„ç¼ºé™·ç å­—æ®µã€‚")
    st.stop()

pat_code = re.compile(rf'(?<!\d){re.escape(code_root)}(?:\.\d+)?(?!\d)', re.I)
try:
    col_as_str = scope_df[code_col].astype(str)
except Exception:
    st.warning(f"åˆ— {code_col} æ— æ³•è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç”¨äºåŒ¹é…ï¼Œè¯·æ£€æŸ¥æ•°æ®ã€‚")
    st.stop()

mask_code = col_as_str.str.contains(pat_code, na=False)
df_code   = scope_df[mask_code].copy()
st.caption(f"ï¼ˆç¼ºé™·ç åˆ—è‡ªåŠ¨è¯†åˆ«ä¸ºï¼š{code_col}ï¼Œå‘½ä¸­ {int(mask_code.sum())} æ¡ï¼‰")

# å…³è”åŸå§‹åº“è¡¥å…¥åŸå§‹ç”Ÿäº§æ—¥æœŸ
raw_hist = _load_raw_excel_store()
if not raw_hist.empty:
    DEDUP_KEYS = [
        "å·¥å•å·","ç´¢èµ”å·","CaseNo","TicketNo",
        "S/N","Serial No","SN","Serial","SerialNo",
        "åºåˆ—å·","åºåˆ—å·SN"
    ]
    join_keys = [k for k in DEDUP_KEYS if (k in df_code.columns) and (k in raw_hist.columns)]
    RAW_PROD_CANDS = ["ç”Ÿäº§æ—¥æœŸ","ç”Ÿäº§æ—¥","ProductionDate","ProdDate","åˆ¶é€ æ—¥æœŸ","å‡ºå‚æ—¥æœŸ"]
    raw_prod_col = next((c for c in RAW_PROD_CANDS if c in raw_hist.columns), None)

    if join_keys and raw_prod_col:
        patch = raw_hist.loc[:, join_keys + [raw_prod_col]].drop_duplicates()
        patch = patch.rename(columns={raw_prod_col: "åŸå§‹ç”Ÿäº§æ—¥æœŸ"})
        df_code = df_code.merge(patch, on=join_keys, how="left", suffixes=("", "_rawdup"))
        df_code["åŸå§‹ç”Ÿäº§æ—¥æœŸ"] = normalize_prod_date_series_to_text(df_code["åŸå§‹ç”Ÿäº§æ—¥æœŸ"])
        norm_text = normalize_prod_date_series_to_text(df_code["åŸå§‹ç”Ÿäº§æ—¥æœŸ"])
        if "_NormProdDate" in df_code.columns:
            df_code["_NormProdDate"] = np.where(df_code["åŸå§‹ç”Ÿäº§æ—¥æœŸ"].notna(), norm_text, df_code["_NormProdDate"])
        else:
            df_code["_NormProdDate"] = norm_text
        dt = parse_any_prod_date_series(df_code["åŸå§‹ç”Ÿäº§æ—¥æœŸ"].astype(str))
        if "Year" not in df_code.columns:  df_code["Year"]  = np.nan
        if "Month" not in df_code.columns: df_code["Month"] = np.nan
        ok = dt.notna()
        df_code.loc[ok, "Year"]  = dt.loc[ok].dt.year
        df_code.loc[ok, "Month"] = dt.loc[ok].dt.month

st.caption(f"ç»Ÿè®¡å£å¾„ï¼š{scope_text} | ç¼ºé™·ç ï¼š{code}")
if df_code.empty:
    st.warning("è¯¥å£å¾„ä¸‹æœªå‘½ä¸­ä»»ä½• 1.1.3.1ï¼ˆæ³„æ¼ï¼‰ç›¸å…³è®°å½•ã€‚")
    st.stop()

# === ä»¥â€œåŸå§‹æ˜ç»†â€çš„åˆå¹¶é€»è¾‘æ„é€ ç»Ÿè®¡åº•è¡¨ df_baseï¼ˆä»…è¡¥åŸå§‹ç”Ÿäº§æ—¥æœŸï¼‰ ===
df_base = df_code.copy()
raw_hist = _load_raw_excel_store()
if not raw_hist.empty:
    DEDUP_KEYS = ["å·¥å•å·","ç´¢èµ”å·","CaseNo","TicketNo","SN","Serial","SerialNo","S/N","Serial No","åºåˆ—å·","åºåˆ—å·SN"]
    join_keys = [k for k in DEDUP_KEYS if (k in df_base.columns) and (k in raw_hist.columns)]
    RAW_PROD_CANDS = ["ç”Ÿäº§æ—¥æœŸ","ç”Ÿäº§æ—¥","ProductionDate","ProdDate","åˆ¶é€ æ—¥æœŸ","å‡ºå‚æ—¥æœŸ"]
    def _pick_first(df, cands):
        for c in cands:
            if c in df.columns:
                return c
        return None
    raw_prod_col = _pick_first(raw_hist, RAW_PROD_CANDS)
    if join_keys and raw_prod_col:
        take_cols = [c for c in join_keys if c in raw_hist.columns]
        if raw_prod_col not in take_cols:
            take_cols.append(raw_prod_col)
        patch = raw_hist.loc[:, take_cols].drop_duplicates().copy()
        if raw_prod_col in join_keys:
            patch["åŸå§‹ç”Ÿäº§æ—¥æœŸ"] = normalize_prod_date_series_to_text(patch[raw_prod_col])
        else:
            patch = patch.rename(columns={raw_prod_col: "åŸå§‹ç”Ÿäº§æ—¥æœŸ"})
            patch["åŸå§‹ç”Ÿäº§æ—¥æœŸ"] = normalize_prod_date_series_to_text(patch["åŸå§‹ç”Ÿäº§æ—¥æœŸ"])
        df_base = df_base.merge(patch, on=join_keys, how="left", suffixes=("", "_rawdup"))

if "åŸå§‹ç”Ÿäº§æ—¥æœŸ" in df_base.columns:
    df_base["_NormProdDate"] = normalize_prod_date_series_to_text(df_base["åŸå§‹ç”Ÿäº§æ—¥æœŸ"]).where(
        df_base["åŸå§‹ç”Ÿäº§æ—¥æœŸ"].notna(), df_base.get("_NormProdDate")
    )

if "_NormProdDate" not in df_code.columns and "åŸå§‹ç”Ÿäº§æ—¥æœŸ" in df_code.columns:
    df_code["_NormProdDate"] = normalize_prod_date_series_to_text(df_code["åŸå§‹ç”Ÿäº§æ—¥æœŸ"])    
prod_year = _parse_prod_year_series(df_code["_NormProdDate"]) if "_NormProdDate" in df_code.columns else pd.Series(np.nan, index=df_code.index)
df_code = df_code.loc[prod_year >= 2024].copy()
if df_code.empty:
    st.warning("æŒ‰è¦æ±‚è¿‡æ»¤åæ— æ•°æ®ï¼›è¯·æ£€æŸ¥ç”Ÿäº§æ—¥æœŸåˆ—æˆ–æ›´æ¢ç»Ÿè®¡å£å¾„ã€‚")
    st.stop()

# â€”â€” å¯é€‰å»é‡ï¼šåŒä¸€å·¥å•/ç´¢èµ”åªä¿ç•™ä¸€æ¡
DEDUP_KEYS = ["å·¥å•å·","ç´¢èµ”å·","CaseNo","TicketNo","SN","Serial","SerialNo"]
dedup_key = next((c for c in DEDUP_KEYS if c in df_code.columns), None)
if dedup_key:
    before = len(df_code)
    df_code = df_code.drop_duplicates(subset=[dedup_key], keep="first")
    st.caption(f"ï¼ˆå·²æŒ‰ {dedup_key} å»é‡ï¼š{before} â†’ {len(df_code)}ï¼‰")

# ================= é¡¶éƒ¨æŠ¬å¤´æ¡ =================
def render_topbar(_code: str, _scope_text: str, df_scope: pd.DataFrame):
    paid = pd.to_numeric(df_scope.get("CostUSD", 0), errors="coerce").fillna(0)
    qty  = len(df_scope)
    cost = float(paid[paid>0].sum())
    avg  = float(paid[paid>0].mean()) if (paid>0).any() else 0.0
    col1,col2,col3,col4 = st.columns([1.2,1,1,1])
    with col1:
        st.markdown(f"""
        <div style="padding:12px 16px;border-radius:14px;background:#0f172a;color:#fff;">
          <div style="font-size:13px;opacity:.7;">ç¼ºé™·ç  / ç»Ÿè®¡å£å¾„</div>
          <div style="font-size:20px;font-weight:700;line-height:1.2;margin-top:2px;">{_code}</div>
          <div style="font-size:13px;margin-top:2px;">{_scope_text}</div>
        </div>
        """, unsafe_allow_html=True)
    with col2: st.metric("è®°å½•æ•°", f"{qty}")
    with col3: st.metric("å·²ä»˜æ¬¾æ€»è´¹ç”¨", money(cost))
    with col4: st.metric("å•å‡å·²ä»˜æ¬¾è´¹ç”¨", money(avg))

render_topbar(code, scope_text, df_code)
st.markdown("---")

# ================= 1.1.3.1.xï¼ˆæ³„æ¼ï¼‰åˆ†é¡¹ =================
def build_1131_subtable(df_in: pd.DataFrame) -> pd.DataFrame:
    TEXT_HINT_CANDS = [
        "Category1","Category2","TAG1","TAG2","TAG3","TAG4",
        "æ•…éšœæè¿°","é—®é¢˜ç°è±¡","ç—‡çŠ¶","ç»´ä¿®å†…å®¹","å¤„ç†æªæ–½","å¤‡æ³¨","æè¿°",
        "Notes","Note","Detail","Details","Description","Comments",
        "åŸå§‹EXCELè¡¨æ˜ å°„","åŸå§‹EXCELæ˜ å°„","åŸå§‹Excelæ˜ å°„","åŸå§‹æ˜ å°„","ç¼ºé™·ç (åŸå§‹)","ç¼ºé™·ç åŸå§‹",
        "Part","PartName","é…ä»¶","é…ä»¶åç§°","å·¥å•å†…å®¹"
    ]
    text_cols = [c for c in [code_col] + TEXT_HINT_CANDS if c in df_in.columns]
    if not text_cols:
        text_cols = list(df_in.columns)

    text = (
        df_in[text_cols]
        .astype(str)
        .apply(lambda s: s.str.replace("\u3000", " ", regex=False))
        .apply(lambda s: s.str.replace(r"\s*[\.\Â·]\s*", ".", regex=True))
        .apply(lambda s: s.str.replace(r"[\s\-_\/]+", " ", regex=True))
    )
    text = text.apply(lambda row: " ".join(row.values), axis=1).str.lower().str.strip()

    def _pat_exact(code: str):
        return re.compile(rf"(?<!\d){re.escape(code)}(?!\d)", re.I)

    code_map = {
        "1.1.3.1.15": ("1.1.3.1.15 Evap Coil","è’¸å‘å™¨"),
        "1.1.3.1.16": ("1.1.3.1.16 Condenser Coil","å†·å‡å™¨"),
        "1.1.3.1.13": ("1.1.3.1.13 L1 leaking points on Suction tube B","è’¸å‘å™¨å‡ºå£å¤„"),
        "1.1.3.1.17": ("1.1.3.1.17 Filter Drier","å¹²ç‡¥è¿‡æ»¤å™¨"),
        "1.1.3.1.2":  ("1.1.3.1.2 Leaking freon in the wall","å†…éƒ¨æ³„æ¼"),
        "1.1.3.1.3":  ("1.1.3.1.3 Found leaking points somewhere and fixed it","ç„Šç‚¹æ³„æ¼"),
        "1.1.3.1.1":  ("1.1.3.1.1 Leak not found/Vacuum and Recharge","æœªæ‰¾åˆ°æ¼ç‚¹"),
        "1.1.3.1.14": ("1.1.3.1.14 L2/L3 leaking points on Suction tube A","å›æ°”ç®¡ç„Šæ¥å¤„"),
        "1.1.3.1.12": ("1.1.3.1.12 H19 leaking points on T Defrost tube B","è’¸å‘å™¨åŒ–éœœåŠ çƒ­ç®¡ç„Šç‚¹"),
        "1.1.3.1.5":  ("1.1.3.1.5 H4/H5 leaking points on Condenser","å†·å‡å™¨ç„Šç‚¹æ³„æ¼"),
        "1.1.3.1.7":  ("1.1.3.1.7 H8/H9/H10 leaking points on Capillary tube A","å¤–éƒ¨æ¯›ç»†ç®¡ç„Šç‚¹æ³„æ¼"),
        "1.1.3.1.4":  ("1.1.3.1.4 H2/H3 leaking points on Discharge Tube Loop","æ•£çƒ­ç®¡ç„Šç‚¹æ³„æ¼"),
        "1.1.3.1.9":  ("1.1.3.1.9 H12/H13 leaking points on Solenoid Valve Normally Open","å¸¸é€šç”µæ± é˜€æ³„æ¼"),
        "1.1.3.1.11": ("1.1.3.1.11 H16/H17/H18 leaking points on Solenoid Valve Normally close","å¸¸é—­ç”µæ± é˜€æ³„æ¼"),
        "1.1.3.1.8":  ("1.1.3.1.8 H11 leaking points on Capillary tube B","è’¸å‘å™¨æ¯›ç»†ç®¡æ³„æ¼"),
        "1.1.3.1.6":  ("1.1.3.1.6 H6/H7 leaking points on Filter drye","å¹²ç‡¥è¿‡æ»¤å™¨é™„è¿‘ç„Šç‚¹"),
    }

    order_codes = sorted(code_map.keys(), key=len, reverse=True)
    unassigned = pd.Series(True, index=df_in.index)
    cost = pd.to_numeric(df_in.get("CostUSD", 0), errors="coerce").fillna(0.0)

    rows = []
    for code in order_codes:
        en, zh = code_map[code]
        m = unassigned & text.str.contains(_pat_exact(code), na=False)
        rows.append({
            "è‹±æ–‡åç§°": en,
            "è´¨é‡é—®é¢˜": zh,
            "æ•°é‡": int(m.sum()),
            "è´¹ç”¨": float(cost[m].sum())
        })
        unassigned &= (~m)

    df_out = pd.DataFrame(rows)
    total_row = {
        "è‹±æ–‡åç§°":"æ€»è®¡",
        "è´¨é‡é—®é¢˜":"",
        "æ•°é‡": int(pd.to_numeric(df_out["æ•°é‡"], errors="coerce").fillna(0).sum()),
        "è´¹ç”¨": float(pd.to_numeric(df_out["è´¹ç”¨"], errors="coerce").fillna(0).sum()),
    }
    df_out = pd.concat([df_out, pd.DataFrame([total_row])], ignore_index=True)

    st.caption("ï¼ˆåˆ†é¡¹åŒ¹é…ä½¿ç”¨åˆ—ï¼š%sï¼‰" % ", ".join(text_cols))
    return df_out

def _keep_positive_rows(df: pd.DataFrame, keep_total=True) -> pd.DataFrame:
    if df is None or df.empty: return df
    out = df.copy()
    if "æ•°é‡" in out.columns:
        out["æ•°é‡"] = pd.to_numeric(out["æ•°é‡"], errors="coerce").fillna(0).astype(int)
        if keep_total:
            mask = (out["æ•°é‡"] > 0) | (out["è‹±æ–‡åç§°"].astype(str) == "æ€»è®¡")
        else:
            mask = (out["æ•°é‡"] > 0)
        out = out[mask]
    return out

df_1131 = build_1131_subtable(df_code)
df_1131 = _keep_positive_rows(df_1131, keep_total=True)
st.markdown("### 1.1.3.1.x åˆ†é¡¹ï¼ˆæ³„æ¼ï¼‰")
if not df_1131.empty:
    show = df_1131.copy(); show["è´¹ç”¨"] = show["è´¹ç”¨"].map(lambda v: f"${float(v):,.0f}")
    st.dataframe(show, use_container_width=True)
    st.download_button("ä¸‹è½½ CSVï¼ˆ1.1.3.1 åˆ†é¡¹ï¼‰",
        data=df_1131.to_csv(index=False, encoding="utf-8-sig"),
        file_name=f"leaking_1131_breakdown_{scope_text.replace(' ','_')}.csv",
        mime="text/csv", use_container_width=True)
else:
    st.info("æš‚æ—  1.1.3.1 ç›¸å…³è®°å½•ã€‚")

# ================= ç»Ÿè®¡è¡¨ï¼ˆæŒ‰ç³»åˆ— / æŒ‰å…·ä½“å‹å·ï¼‰ =================
USE_MONTH_CAND = [
    "å·²ä½¿ç”¨ä¿ä¿®æ—¶é•¿","å·²ä½¿ç”¨ä¿ä¿®æœˆæ•°","ä¿ä¿®å·²ä½¿ç”¨æ—¶é•¿","å·²ä½¿ç”¨æœˆä»½","ä¿ä¿®ä½¿ç”¨æœˆæ•°",
    "ä½¿ç”¨æœˆæ•°","ä½¿ç”¨æœˆ","ServiceMonths","months_in_use","use_months"
]
SERIES_PRI_CAND = ["å‹å·å½’ç±»","å‹å·ç³»åˆ—","series","model_series"]
SERIES_SEC_CAND = ["æœºå‹","äº§å“å‹å·","MODEL","Model"]
SPEC_MODEL_CAND = ["å‹å·","å…·ä½“å‹å·","æœºå‹æ˜ç»†","MODEL","Model","Part Model","PartModel","Part"]

MODEL_CATEGORY_MAP = {
    "MBF":"å†°ç®±","MCF":"é™ˆåˆ—æŸœ","MSF":"æ²™æ‹‰å°","MGF":"å·¥ä½œå°","MPF":"ç»ç’ƒå°","CHEF BASE":"æŠ½å±‰å·¥ä½œå°",
    "MBC":"æ»‘ç›–å†·è—æŸœ","MBB":"å§å¼å§å°","SBB":"ç§»é—¨å§å°","MKC":"å•¤é…’æŸœ","MBGF":"æ»‘ç›–å†·å†»æŸœ",
    "AMC":"ç‰›å¥¶æŸœ","RDCS":"æ–¹å½¢å±•ç¤ºæŸœ","CRDC":"å¼§å½¢å±•ç¤ºæŸœ","CRDS":"å°é¢æ–¹å½¢å±•ç¤ºæŸœ","ATHOM":"é¥®æ–™æŸœ",
    "AOM":"ç«‹å¼é¥®æ–™æŸœ","MSCT":"é…æ–™å°",
}
MODEL_ORDER = ["AOM","CHEF BASE","MBC","MBF","MBGF","MCF","MGF","MPF","MSF"]
ALIASES = {"CHEFBASE":"CHEF BASE","CHEF-BASE":"CHEF BASE","CHEF_BASE":"CHEF BASE","CHEF BASE":"CHEF BASE"}

def resolve_model_key(raw: str) -> str:
    s=(raw or "").strip().replace("\u3000", " ")
    s_norm=re.sub(r"[\s\-_\/]+", " ", s).strip()
    key_up=s_norm.upper().replace(" ","")
    for k,v in ALIASES.items():
        if key_up==k.upper().replace(" ",""): return v
    toks=s_norm.split()
    if len(toks)>1: return " ".join(t.upper() for t in toks)
    return s_norm.upper()

def _pick_col(df: pd.DataFrame, cands: list):
    for c in cands:
        if c in df.columns:
            return c
    norm = {re.sub(r"[\s_]+", "", str(c).lower()): c for c in df.columns}
    for want in cands:
        k = re.sub(r"[\s_]+", "", str(want).lower())
        if k in norm:
            return norm[k]
    return None

def load_model_fleet():
    if MODEL_FLEET_JSON.exists():
        try: return json.loads(MODEL_FLEET_JSON.read_text(encoding="utf-8"))
        except Exception: pass
    return {}

def get_fleet_month(fleet: dict, year: int=None, month: int=None)->dict:
    if not fleet: return {k:0 for k in MODEL_CATEGORY_MAP.keys()}
    if year is None or month is None:
        try:
            yy=max(int(y) for y in fleet.keys())
            mm=max(int(m) for m in fleet[str(yy)].keys())
            year,month=yy,mm
        except Exception:
            return {k:0 for k in MODEL_CATEGORY_MAP.keys()}
    y,m=str(int(year)),str(int(month))
    return {k:int(fleet.get(y,{}).get(m,{}).get(k,0)) for k in MODEL_CATEGORY_MAP.keys()}

def _bucket_series(months: pd.Series):
    m=pd.to_numeric(months, errors="coerce").where(lambda x: x>0)
    return pd.cut(m, bins=[0,4,8,np.inf], labels=["çŸ­æœŸï¼ˆ1-4æœˆï¼‰","ä¸­æœŸï¼ˆ5-8æœˆï¼‰","é•¿æœŸï¼ˆ9-24æœˆï¼‰"],
                  include_lowest=True, right=True)

def build_tables(df_subset: pd.DataFrame, *, year=None, month=None):
    months_col = _pick_col(df_subset, USE_MONTH_CAND)
    series_col = _pick_col(df_subset, SERIES_PRI_CAND) or _pick_col(df_subset, SERIES_SEC_CAND)
    spec_col   = _pick_col(df_subset, SPEC_MODEL_CAND)

    if months_col is None:
        return pd.DataFrame(), pd.DataFrame()

    if series_col is None and spec_col is not None:
        tmp = df_subset[spec_col].astype(str).str.upper().str.extract(r"^([A-Z]{2,4})")
        df_subset = df_subset.copy()
        df_subset["__SERIES__"] = tmp[0]
        series_col = "__SERIES__"

    m = pd.to_numeric(df_subset[months_col], errors="coerce")
    valid_mask = m.notna() & (m>0)
    df_valid = df_subset[valid_mask].copy()
    if df_valid.empty:
        return pd.DataFrame(), pd.DataFrame()
    df_valid["__bucket__"] = _bucket_series(df_valid[months_col])

    # è¡¨1ï¼šæŒ‰ç³»åˆ—
    tbl1=pd.DataFrame()
    if series_col is not None:
        g = pd.pivot_table(df_valid, index=series_col, columns="__bucket__", values="_NormProdDate",
                           aggfunc="count", fill_value=0)
        for c in ["çŸ­æœŸï¼ˆ1-4æœˆï¼‰","ä¸­æœŸï¼ˆ5-8æœˆï¼‰","é•¿æœŸï¼ˆ9-24æœˆï¼‰"]:
            if c not in g.columns: g[c]=0
        g = g[["çŸ­æœŸï¼ˆ1-4æœˆï¼‰","ä¸­æœŸï¼ˆ5-8æœˆï¼‰","é•¿æœŸï¼ˆ9-24æœˆï¼‰"]]
        g["æ€»è®¡"]=g.sum(axis=1)
        g.reset_index(inplace=True)
        g.rename(columns={series_col:"å‹å·"}, inplace=True)

        fleet=load_model_fleet()
        month_fleet=get_fleet_month(fleet, year, month)
        def _fleet_lookup(s):
            key=resolve_model_key(s)
            return int(month_fleet.get(key,0))
        g["åœ¨ä¿é‡"]=g["å‹å·"].map(_fleet_lookup).fillna(0).astype(int)
        g["é—®é¢˜æ¯”ä¾‹(%)"]=np.where(g["åœ¨ä¿é‡"]>0, g["æ€»è®¡"]/g["åœ¨ä¿é‡"]*100.0, np.nan)

        order_map={k:i for i,k in enumerate(MODEL_ORDER)}
        g["__ord__"]=g["å‹å·"].map(order_map).fillna(9999)
        g=g.sort_values(["__ord__","æ€»è®¡"], ascending=[True,False]).drop(columns="__ord__")
        tbl1=g

    # è¡¨2ï¼šæŒ‰å…·ä½“å‹å·
    tbl2=pd.DataFrame()
    if spec_col is not None:
        g2 = pd.pivot_table(df_valid, index=spec_col, columns="__bucket__", values="_NormProdDate",
                            aggfunc="count", fill_value=0)
        for c in ["çŸ­æœŸï¼ˆ1-4æœˆï¼‰","ä¸­æœŸï¼ˆ5-8æœˆï¼‰","é•¿æœŸï¼ˆ9-24æœˆï¼‰"]:
            if c not in g2.columns: g2[c]=0
        g2 = g2[["çŸ­æœŸï¼ˆ1-4æœˆï¼‰","ä¸­æœŸï¼ˆ5-8æœˆï¼‰","é•¿æœŸï¼ˆ9-24æœˆï¼‰"]]
        g2["æ€»è®¡"]=g2.sum(axis=1)
        g2.reset_index(inplace=True)
        g2.rename(columns={spec_col:"å…·ä½“å‹å·"}, inplace=True)

        g2["åœ¨ä¿é‡"]=g2["å…·ä½“å‹å·"].map(lookup_spec_fleet).fillna(0).astype(int)
        g2["æ¯”ä¾‹(%)"]=np.where(g2["åœ¨ä¿é‡"]>0, g2["æ€»è®¡"]/g2["åœ¨ä¿é‡"]*100.0, np.nan)
        g2=g2.sort_values("æ€»è®¡", ascending=False)
        tbl2=g2

    return tbl1, tbl2

# ========= æ–°å¢ï¼šæ„å»ºâ€œæ˜ç»†ç´¢å¼•ï¼ˆä¸‰åˆ—ï¼‰â€çš„é€šç”¨å‡½æ•° =========
SERIAL_CANDS_MAIN = ["åºåˆ—å·","S/N","SN","Serial No","SerialNo","Serial","åºåˆ—å·SN"]
SPEC_MODEL_CAND_ALL = ["äº§å“å‹å·","å…·ä½“å‹å·","å‹å·","æœºå‹æ˜ç»†","MODEL","Model","Part Model","PartModel","Part"]

def build_companion_index(df_src: pd.DataFrame) -> pd.DataFrame:
    if df_src is None or df_src.empty:
        return pd.DataFrame(columns=["äº§å“å‹å·","åºåˆ—å·","åŸå§‹ç”Ÿäº§æ—¥æœŸ"])
    # äº§å“å‹å·
    model_col = next((c for c in SPEC_MODEL_CAND_ALL if c in df_src.columns), None)
    if model_col is None:
        # é€€åŒ–ä¸ºç³»åˆ—æˆ–å…¶å®ƒå¯ç”¨åˆ—
        model_col = next((c for c in ["å…·ä½“å‹å·","å‹å·","å‹å·ç³»åˆ—","series","model_series"] if c in df_src.columns), None)
    # åºåˆ—å·
    serial_col = next((c for c in SERIAL_CANDS_MAIN if c in df_src.columns), None)
    # åŸå§‹ç”Ÿäº§æ—¥æœŸ
    prod_col = "åŸå§‹ç”Ÿäº§æ—¥æœŸ" if "åŸå§‹ç”Ÿäº§æ—¥æœŸ" in df_src.columns else ("_NormProdDate" if "_NormProdDate" in df_src.columns else None)

    out = pd.DataFrame({
        "äº§å“å‹å·": (df_src[model_col].astype(str) if model_col else ""),
        "åºåˆ—å·":   (df_src[serial_col].astype(str) if serial_col else ""),
        "åŸå§‹ç”Ÿäº§æ—¥æœŸ": (normalize_prod_date_series_to_text(df_src[prod_col]) if prod_col else "")
    })
    # è¿‡æ»¤ç©ºç™½ & å»é‡
    def _nonempty_row(r):
        return any([str(r["äº§å“å‹å·"]).strip(), str(r["åºåˆ—å·"]).strip(), str(r["åŸå§‹ç”Ÿäº§æ—¥æœŸ"]).strip()])
    out = out[out.apply(_nonempty_row, axis=1)]
    out = out.drop_duplicates().reset_index(drop=True)
    return out

# ========= æ–°å¢ï¼šå¹¶æ’æ¸²æŸ“ï¼ˆå·¦åŸè¡¨ / å³æ˜ç»†ç´¢å¼•ï¼‰ =========
def render_table_pair(title: str, df_stats: pd.DataFrame, order_cols, df_for_index: pd.DataFrame):
    col_left, col_right = st.columns([2, 1], vertical_alignment="top")
    with col_left:
        cols=[c for c in order_cols if c in df_stats.columns]
        view=df_stats.loc[:, cols].copy() if cols else df_stats.copy()
        for pct_col in ["é—®é¢˜æ¯”ä¾‹(%)","æ¯”ä¾‹(%)"]:
            if pct_col in view.columns:
                view[pct_col] = view[pct_col].map(lambda x: "" if pd.isna(x) else f"{float(x):.2f}%")
        st.subheader(title)
        st.dataframe(view, use_container_width=True)
    with col_right:
        st.subheader("æ˜ç»†ç´¢å¼•")
        idx_df = build_companion_index(df_for_index)
        if idx_df.empty:
            st.info("æ— å¯å±•ç¤ºçš„ äº§å“å‹å·/åºåˆ—å·/åŸå§‹ç”Ÿäº§æ—¥æœŸã€‚")
        else:
            st.dataframe(idx_df[["äº§å“å‹å·","åºåˆ—å·","åŸå§‹ç”Ÿäº§æ—¥æœŸ"]], use_container_width=True, height=min(400, 42 + 28*min(len(idx_df), 10)))

# â€”â€” ç”Ÿæˆç»Ÿè®¡è¡¨å¹¶æ¸²æŸ“å¹¶æ’è§†å›¾
t1, t2 = build_tables(df_code, year=year, month=month_for_fleet)
base1 = pd.DataFrame(columns=["å‹å·","çŸ­æœŸï¼ˆ1-4æœˆï¼‰","ä¸­æœŸï¼ˆ5-8æœˆï¼‰","é•¿æœŸï¼ˆ9-24æœˆï¼‰","æ€»è®¡","åœ¨ä¿é‡","é—®é¢˜æ¯”ä¾‹(%)"])
base2 = pd.DataFrame(columns=["å…·ä½“å‹å·","çŸ­æœŸï¼ˆ1-4æœˆï¼‰","ä¸­æœŸï¼ˆ5-8æœˆï¼‰","é•¿æœŸï¼ˆ9-24æœˆï¼‰","æ€»è®¡","åœ¨ä¿é‡","æ¯”ä¾‹(%)"])

render_table_pair(
    "æ³„æ¼ï¼ˆæŒ‰ç³»åˆ—ï¼‰",
    (t1 if not t1.empty else base1),
    order_cols=("å‹å·","çŸ­æœŸï¼ˆ1-4æœˆï¼‰","ä¸­æœŸï¼ˆ5-8æœˆï¼‰","é•¿æœŸï¼ˆ9-24æœˆï¼‰","æ€»è®¡","åœ¨ä¿é‡","é—®é¢˜æ¯”ä¾‹(%)"),
    df_for_index=df_code
)
render_table_pair(
    "æ³„æ¼ï¼ˆæŒ‰å…·ä½“å‹å·ï¼‰",
    (t2 if not t2.empty else base2),
    order_cols=("å…·ä½“å‹å·","çŸ­æœŸï¼ˆ1-4æœˆï¼‰","ä¸­æœŸï¼ˆ5-8æœˆï¼‰","é•¿æœŸï¼ˆ9-24æœˆï¼‰","æ€»è®¡","åœ¨ä¿é‡","æ¯”ä¾‹(%)"),
    df_for_index=df_code
)

# ===== åˆ†é¡¹æ·±å…¥åˆ†æï¼ˆä»…æ•°é‡>0ï¼Œé»˜è®¤åªå±•å¼€ç¬¬ä¸€é¡¹ï¼‰=====
st.markdown("## åˆ†é¡¹æ·±å…¥åˆ†æï¼ˆä»…æ•°é‡>0ï¼‰")

def _clean_join(df_):
    j = df_.astype(str).agg(" ".join, axis=1).str.lower().str.replace("\u3000", " ", regex=False)
    return j.str.replace(r"[\s\-_\/]+", " ", regex=True).str.strip()

def _subset_1131_item(df_src: pd.DataFrame, en_name: str) -> pd.DataFrame:
    """
    ä»â€œè‹±æ–‡åç§°â€ä¸­ç²¾ç¡®æŠ½å–å®Œæ•´å­ç ï¼ˆå¦‚ 1.1.3.1.15ï¼‰ï¼Œ
    å†åœ¨åŸå§‹æ–‡æœ¬é‡Œåšç²¾å‡†åŒ¹é…ï¼Œé¿å…æŠŠ 15 è¯¯å½“æˆ 1 çš„å‰ç¼€ã€‚
    """
    if df_src is None or df_src.empty:
        return df_src.iloc[0:0].copy()

    # 1) ä»æ ‡é¢˜é‡ŒæŠ½å–å®Œæ•´ codeï¼šä¼˜å…ˆæ­£åˆ™ï¼ˆè¡Œé¦–ï¼‰ï¼Œé€€åŒ–åˆ°ç¬¬ä¸€ä¸ª token
    code_pat = re.compile(r"^1\.1\.3\.1\.(?:1[0-7]|[1-9])\b", re.I)
    m = code_pat.search(en_name or "")
    code_full = m.group(0) if m else (str(en_name).strip().split()[0] if en_name else "")

    if not code_full:
        return df_src.iloc[0:0].copy()

    # 2) æ‹¼æ¥åŸå§‹æ–‡æœ¬åšåŒ¹é…ï¼ˆä¿æŒä½ åŸæ¥çš„æ¸…æ´—ï¼‰
    def _clean_join(df_):
        j = df_.astype(str).agg(" ".join, axis=1).str.lower().str.replace("\u3000", " ", regex=False)
        return j.str.replace(r"[\s\-_\/]+", " ", regex=True).str.strip()

    j = _clean_join(df_src)

    # 3) ç”¨â€œç²¾ç¡® codeâ€åšè¾¹ç•ŒåŒ¹é…ï¼Œç¡®ä¿ä¸è¯¯ä¼¤ 1.1.3.1.1 vs 1.1.3.1.15
    pat = re.compile(rf"(?<![0-9a-z]){re.escape(code_full)}(?![0-9a-z])", re.I)
    return df_src[j.str.contains(pat, na=False)].copy()


rows_need = (
    df_1131[
        df_1131["è‹±æ–‡åç§°"].astype(str).str.match(r"^1\.1\.3\.1\.(?:[1-9]|1[0-7])\b", na=False) &
        (pd.to_numeric(df_1131["æ•°é‡"], errors="coerce").fillna(0) > 0)
    ]
    .sort_values("æ•°é‡", ascending=False)
    .copy()
)

if rows_need.empty:
    st.info("å½“å‰ 1.1.3.1.x æ²¡æœ‰æ•°é‡>0 çš„æ¡ç›®å¯æ·±å…¥åˆ†æã€‚")
else:
    for idx, r in rows_need.iterrows():
        en_name = str(r["è‹±æ–‡åç§°"]); zh_name = str(r.get("è´¨é‡é—®é¢˜",""))
        sub_df = _subset_1131_item(df_code, en_name)
        with st.expander(f"ã€{en_name}ã€‘{zh_name}ï½œè®°å½• {int(r['æ•°é‡'])}ï¼Œè´¹ç”¨ {money(float(r.get('è´¹ç”¨',0)))}",
                         expanded=(idx==rows_need.index[0])):
            a1, a2 = build_tables(sub_df, year=year, month=month_for_fleet)
            _b1 = pd.DataFrame(columns=["å‹å·","çŸ­æœŸï¼ˆ1-4æœˆï¼‰","ä¸­æœŸï¼ˆ5-8æœˆï¼‰","é•¿æœŸï¼ˆ9-24æœˆï¼‰","æ€»è®¡","åœ¨ä¿é‡","é—®é¢˜æ¯”ä¾‹(%)"])
            _b2 = pd.DataFrame(columns=["å…·ä½“å‹å·","çŸ­æœŸï¼ˆ1-4æœˆï¼‰","ä¸­æœŸï¼ˆ5-8æœˆï¼‰","é•¿æœŸï¼ˆ9-24æœˆï¼‰","æ€»è®¡","åœ¨ä¿é‡","æ¯”ä¾‹(%)"])
            # â€”â€” æ¯ä¸ªåˆ†é¡¹é‡Œçš„ä¸¤å¼ ç»Ÿè®¡è¡¨ä¹ŸåŠ â€œé½å¹³çš„æ˜ç»†ç´¢å¼•â€
            render_table_pair("æŒ‰ç³»åˆ—", (a1 if not a1.empty else _b1),
                order_cols=("å‹å·","çŸ­æœŸï¼ˆ1-4æœˆï¼‰","ä¸­æœŸï¼ˆ5-8æœˆï¼‰","é•¿æœŸï¼ˆ9-24æœˆï¼‰","æ€»è®¡","åœ¨ä¿é‡","é—®é¢˜æ¯”ä¾‹(%)"),
                df_for_index=sub_df)
            render_table_pair("æŒ‰å…·ä½“å‹å·", (a2 if not a2.empty else _b2),
                order_cols=("å…·ä½“å‹å·","çŸ­æœŸï¼ˆ1-4æœˆï¼‰","ä¸­æœŸï¼ˆ5-8æœˆï¼‰","é•¿æœŸï¼ˆ9-24æœˆï¼‰","æ€»è®¡","åœ¨ä¿é‡","æ¯”ä¾‹(%)"),
                df_for_index=sub_df)

# ================= åŸå§‹æ˜ç»†ï¼ˆå‘½ä¸­ 1.1.3.1 çš„è®°å½•ï¼‰ =================
with st.expander("åŸå§‹æ˜ç»†ï¼ˆå‘½ä¸­ 1.1.3.1 çš„è®°å½•ï¼‰", expanded=False):
    base_cols = [
        "Category1","Category2","TAG1","TAG2","TAG3","TAG4",
        "CostUSD","PaidQty","IsWarranty","Completed"
    ]
    base_cols = [c for c in base_cols if c in df_base.columns]

    RAW_PROD_CANDS = ["ç”Ÿäº§æ—¥æœŸ","ç”Ÿäº§æ—¥","ProductionDate","ProdDate","åˆ¶é€ æ—¥æœŸ","å‡ºå‚æ—¥æœŸ"]
    def _pick_first(df, cands):
        for c in cands:
            if c in df.columns:
                return c
        return None

    df_view = df_base.copy()

    raw_hist = _load_raw_excel_store()
    if not raw_hist.empty:
        DEDUP_KEYS = ["å·¥å•å·","ç´¢èµ”å·","CaseNo","TicketNo","SN","Serial","SerialNo","S/N","Serial No","åºåˆ—å·","åºåˆ—å·SN"]
        join_keys = [k for k in DEDUP_KEYS if (k in df_view.columns) and (k in raw_hist.columns)]
        raw_prod_col = _pick_first(raw_hist, RAW_PROD_CANDS)
        if join_keys and raw_prod_col:
            take_cols = join_keys[:]
            if raw_prod_col not in take_cols:
                take_cols.append(raw_prod_col)
            patch = raw_hist.loc[:, take_cols].drop_duplicates()
            patch = patch.rename(columns={raw_prod_col: "åŸå§‹ç”Ÿäº§æ—¥æœŸ"})
            df_view = df_view.merge(patch, on=join_keys, how="left", suffixes=("", "_rawdup"))
            st.caption("ï¼ˆå·²ä»ğŸ“ åŸå§‹ Excel è¡¨æ˜ å°„æŒ‰ %s å…³è”è¡¥å…¥ï¼šåŸå§‹ç”Ÿäº§æ—¥æœŸï¼‰" % ", ".join(join_keys))
    else:
        st.caption("ï¼ˆæœªæ‰¾åˆ°ğŸ“ åŸå§‹ Excel è¡¨æ˜ å°„ï¼Œæ˜¾ç¤ºç°æœ‰åˆ—ï¼‰")

    if "åŸå§‹ç”Ÿäº§æ—¥æœŸ" in df_view.columns:
        df_view["åŸå§‹ç”Ÿäº§æ—¥æœŸ"] = normalize_prod_date_series_to_text(df_view["åŸå§‹ç”Ÿäº§æ—¥æœŸ"])

    serial_src = next((c for c in ["åºåˆ—å·","S/N","SN","Serial No","SerialNo","Serial","åºåˆ—å·SN"] if c in df_view.columns), None)
    df_view["åºåˆ—å·"] = df_view[serial_src].astype(str) if serial_src else ""

    show_cols = []
    if "åŸå§‹ç”Ÿäº§æ—¥æœŸ" in df_view.columns: show_cols.append("åŸå§‹ç”Ÿäº§æ—¥æœŸ")
    show_cols.append("åºåˆ—å·")
    show_cols.extend([c for c in base_cols if c not in show_cols])

    st.caption("æç¤ºï¼šå·²å»é™¤â€œåŸå§‹åºåˆ—å·â€åˆ—ï¼›ä»…å±•ç¤ºç»Ÿä¸€ã€åºåˆ—å·ã€‘ã€‚")
    st.dataframe(df_view.loc[:, show_cols], use_container_width=True)

    st.download_button(
        "ä¸‹è½½ CSVï¼ˆåŸå§‹æ˜ç»†ï¼‰",
        data=df_view.loc[:, show_cols].to_csv(index=False, encoding="utf-8-sig"),
        file_name="leaking_1131_raw_details.csv",
        mime="text/csv",
        use_container_width=True
    )
